# -*- coding: utf-8 -*-
"""Copy of Train a GPT-2 Text-Generating Model w/ GPU

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y_KrkEhNlTh6fmg1wbVcL0FmCWsj643S
"""

!pip install -q gpt-2-simple
import gpt_2_simple as gpt2
from datetime import datetime
from google.colab import files
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

"""## GPU

Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.

You can verify which GPU is active by running the cell below.
"""

!nvidia-smi

"""## Downloading GPT-2

If you're retraining a model on new text, you need to download the GPT-2 model first. 



* `124M` (default): the "small" model, 500MB on disk.

"""

gpt2.download_gpt2(model_name="124M")

"""## Mounting Google Drive


"""

gpt2.mount_gdrive()

"""## Uploading a Text File to be Trained to Colaboratory


"""

file_name = "paridhi_train.txt"

gpt2.copy_file_from_gdrive(file_name)

"""## Finetune GPT-2

The next cell will start the actual finetuning of GPT-2.

The model checkpoints will be saved in `/checkpoint/run1` by default. 
"""

tf.compat.v1.reset_default_graph()
sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset=file_name,
              model_name='124M',
              steps=4000,
              restore_from='latest',
              run_name='run1',
              print_every=1000,
              learning_rate=1e-4,
              sample_every=1000,
              overwrite=True,
              
              
              
              
              
              )

gpt2.copy_checkpoint_to_gdrive(run_name='run1')

"""## Load a Trained Model Checkpoint

Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM.
"""

gpt2.copy_checkpoint_from_gdrive(run_name='run1')

tf.compat.v1.reset_default_graph()
sess = gpt2.start_tf_sess() 
gpt2.load_gpt2(sess, run_name='run1')

"""## Generate Text From The Trained Model

After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model.
"""

gpt2.generate(sess,
              length=500,
              temperature=0.7,
              top_k=40,
              top_p=0.9,
              
              truncate='<|endoftext|>',
              prefix="Artificial Intelligence for automobiles",
              nsamples=1,
              batch_size=1
              )

gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())

gpt2.generate_to_file(sess,
                      destination_path=gen_file,
                      length=500,
                      temperature=0.7,
                      nsamples=1,
                      batch_size=1
                      )

# may have to run twice to get file to download
files.download(gen_file)

!kill -9 -1